<html><head><link rel="stylesheet" href="/sssd-test2/trac.css" type="text/css" /><link rel="stylesheet" href="/sssd-test2/wiki.css" type="text/css" /></head><body><p>
</p><div class="wiki-toc">
<ol>
  <li>
    <a href="#FastNSSCache">FastNSSCache</a>
    <ol>
      <li>
        <a href="#ProblemStatement">Problem Statement</a>
      </li>
      <li>
        <a href="#OverviewofFastNSSCachesolution">Overview of FastNSSCache solution</a>
      </li>
      <li>
        <a href="#Implementationdetails">Implementation details</a>
      </li>
      <li>
        <a href="#Configuration">Configuration</a>
      </li>
      <li>
        <a href="#FutureImprovements">Future Improvements</a>
      </li>
    </ol>
  </li>
</ol>
</div><p>
</p>
<h1 id="FastNSSCache">FastNSSCache</h1>
<h2 id="ProblemStatement">Problem Statement</h2>
<p>
Currently in SSSD user and group lookups are performed by using a connection to a named socket.
This means each client has to talk to the sssd_nss daemon for every lookup it needs to perform.
</p>
<p>
Although this behaviour works well for normal machines, it has scalability limits on busy machines with many processes that need to query the user/group database frequently.
</p>
<p>
There are 2 factors that cause scalability issues:
</p>
<ol><li>context switches
</li><li>the responder process is single-threaded (although asynchronous) so the amount of processing it can do is limited by the speed of 1 cpu
</li></ol><p>
Each request suffers from at least 2 context switches (and a few copies of the data in memory) to write() the request in, wait until it is processed by sssd_nss, read() the reply back.
Because sssd_nss may be busy answering many requests a queue may build up and
replies be delayed.
</p>
<p>
Allowing the clinets to direcly access SSSD caches is not possible for various
reasons including:
</p>
<ul><li>sssd uses LDB as caching backend and LDB depends on byte range locks. Letting a client read access to the cache would allow DoS if the client lcoks a record and never unlock it.
</li><li>sssd stores data not all clients are allowed to get access to (password hashes for example) and partitioning access to this data within the LDB cache is not feasible.
</li></ul><p>
A method to avoid context switches and the sssd_nss bottleneck without compromising th security of the system is therefore desirable.
</p>
<h2 id="OverviewofFastNSSCachesolution">Overview of FastNSSCache solution</h2>
<p>
The FastNSSCache feature addresses both issues.
</p>
<p>
This is done by creating a specialized cache that have a few properties
</p>
<ul><li>Contains only public data (the same data available in a public passwd or group files)
</li><li>Read only for clients
</li><li>Does not use locking and yet prevents access to inconsistent data
</li><li>Cache has a fixed size and uses a FIFO (for now) method to know which entries to purge
</li><li>Fallback to named sockets if entry is not found in the Fast Cache
</li></ul><h2 id="Implementationdetails">Implementation details</h2>
<p>
The cache files are opened on the client at the first query and mmaped in the
process memory, all accesses to data are therefore direct access to memory and
do not suffer from any context switch. They also happen in parallel within
each process with synchronization (in order to allow updates) performed by
using memory barriers.
</p>
<p>
Cache files can only be used for direct lookups (by name or by uid/gid),
enumerations are _never_ handled via fast cache lookups by design, they always
fallback to socket communication.
</p>
<p>
The "maps" currently available are the _passwd_ and _group_ maps, each map has
a file associated in the /var/lib/sss/mc directory which is accessible
read-only by clients.
</p>
<h2 id="Configuration">Configuration</h2>
<p>
At the moement we plan to provide 3 parameters per map that can control the caches.
</p>
<ul><li>Per map enablement parameter that allows to activate/deactiveate maps invidiually.
</li><li>Per map cache size to fine tune the cache sizes in case space is at a premium or the dataset does not fit the default cache.
</li><li>Expiration time for entries.
</li></ul><p>
Cache entries warrant a shorter expiration time than current LDB caches because access to these entries is undetectable by sssd_nss which cannot decide how much an entry is required and whether a midway refresh is needed. By shortening the FastNSSCache entries life time we incur in the penalty of using the pipe from time to time but in turn we allow ssd_nss to decide whether it is required to refresh the entry or not.
</p>
<h2 id="FutureImprovements">Future Improvements</h2>
<ul><li>Better garbage collection on the server side, at the moment a FIFO is used.
</li><li>Handle caching other nsswitch.conf database plugins in order to avoid slow access to the files db
</li></ul></body></html>